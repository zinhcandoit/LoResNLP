#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
PreP-OCR Data Generation Tool (å¹¶è¡Œç‰ˆæœ¬)
=====================================

æ­¤å·¥å…·é€šè¿‡ä»¥ä¸‹æ­¥éª¤ç”ŸæˆåˆæˆOCRè®­ç»ƒæ•°æ®ï¼š
1. å¹¶è¡Œåˆ›å»ºå¹²å‡€çš„åŸºç¡€å›¾åƒ
2. ä¸ºæ¯ä¸ªåŸºç¡€å›¾åƒå¹¶è¡Œæ·»åŠ 4ä¸ªçº§åˆ«çš„å™ªå£°/åŠ£åŒ–
3. ç”Ÿæˆå¯¹åº”çš„çœŸå€¼æ–‡ä»¶

ç‰¹æ€§ï¼š
- å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ï¼Œç”Ÿæˆé€Ÿåº¦æ›´å¿«
- å®žæ—¶ç»Ÿè®¡çš„è¿›åº¦æ¡æ˜¾ç¤º
- å¯é…ç½®å·¥ä½œçº¿ç¨‹æ•°é‡
- çº¿ç¨‹å®‰å…¨çš„æ–‡ä»¶æ“ä½œ

ä½¿ç”¨æ–¹æ³•ï¼š
    python generate_ocr_data.py --base 5                    # ç”Ÿæˆ5ä¸ªåŸºç¡€å›¾åƒ + 20ä¸ªå™ªå£°å˜ä½“
    python generate_ocr_data.py --base 10 --workers 8       # ä½¿ç”¨8ä¸ªå·¥ä½œçº¿ç¨‹
    python generate_ocr_data.py --base 100 --workers 16     # å¤§æ‰¹é‡ç”Ÿæˆï¼Œä½¿ç”¨16ä¸ªå·¥ä½œçº¿ç¨‹

æ€§èƒ½è¯´æ˜Žï¼š
- é»˜è®¤å·¥ä½œçº¿ç¨‹æ•°: min(32, CPUæ ¸å¿ƒæ•° + 4)
- I/Oå¯†é›†åž‹æ“ä½œä»Žæ›´å¤šå·¥ä½œçº¿ç¨‹ä¸­å—ç›Šæ›´å¤š
- æ›´å¤šå·¥ä½œçº¿ç¨‹ä¼šå› PILå›¾åƒå¯¹è±¡å¢žåŠ å†…å­˜ä½¿ç”¨é‡
"""

import random
import argparse
from pathlib import Path
from typing import Tuple, List
import sys
import os
from PIL import Image
import concurrent.futures
import threading
import time
from tqdm import tqdm

random.seed(42)
# Add function directory to path
sys.path.append(str(Path('./function')))

from generate_base_add_noise import generate_base_image, add_noise_and_reduce_resolution, binarize_image
from extract_label import parse_label_line
class OCRDataGenerator:
    """OCR synthetic data generator"""
    
    def __init__(self, max_workers=None):
        """Initialize the generator"""
        self.label_file = Path("./data/midterm/final/T1/Label.txt")
        self.output_folder = Path("./data/output_sft")
        self.setup_directories()
        self.base_images = []  # Store base images for noise processing
        # ä¼˜åŒ–workeræ•°é‡ï¼šå¯¹äºŽI/Oå¯†é›†åž‹ä»»åŠ¡ï¼Œå»ºè®®ä½¿ç”¨CPUæ ¸å¿ƒæ•°çš„1-2å€
        # ä½†ä¸è¶…è¿‡16ä¸ªï¼Œé¿å…è¿‡åº¦ç«žäº‰
        cpu_count = len(os.sched_getaffinity(0)) if hasattr(os, 'sched_getaffinity') else os.cpu_count() or 1
        self.max_workers = max_workers or min(16, cpu_count)
        self.lock = threading.Lock()
    
    def setup_directories(self):
        """Create output directories"""
        self.clean_dir = self.output_folder / "clean"
        self.noisy_dir = self.output_folder / "noisy" 
        self.gt_dir = self.output_folder / "ground_truth"
        
        for dir_path in [self.clean_dir, self.noisy_dir, self.gt_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    def get_text_chunk_that_fits(self, max_attempts: int = 10) -> Tuple[str, str, str]:
        """Get a text chunk that will fit properly in the generated image"""
        text_files = list(self.text_folder.glob("*.txt"))
        if not text_files:
            raise FileNotFoundError(f"No text files found in {self.text_folder}")
        
        for attempt in range(max_attempts):
            # Select random text file and chunk
            text_file = random.choice(text_files)
            with open(text_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # 90% chance: normal length (15-25 lines)
            # 10% chance: varied length (1 to 45 lines)
            if random.random() < 0.9:
                # Normal length
                lines_per_image = random.randint(15, 45)
            else:
                # Varied length for diversity
                lines_per_image = random.randint(1, 45)
            
            if len(lines) >= lines_per_image:
                start_idx = random.randint(0, len(lines) - lines_per_image)
                selected_lines = lines[start_idx:start_idx + lines_per_image]
            else:
                selected_lines = lines
            
            # Format text with conservative indentation
            formatted_lines = []
            for line in selected_lines:
                line = line.strip()
                if line == "":
                    continue  # Skip empty lines to save space
                # Add modest indentation for new paragraphs
                if len(formatted_lines) > 0 and random.random() < 0.3:
                    formatted_lines.append(" " * random.randint(2, 4) + line)
                else:
                    formatted_lines.append(line)
            
            text_content = "\n".join(formatted_lines).strip()
            
            # Validate text content
            if text_content and len(text_content) > 10:  # Lower threshold for short texts
                return text_content, text_file.stem, f"attempt_{attempt}_lines_{lines_per_image}"
        
        # Fallback: use a simple text chunk
        return "Sample text for OCR training data generation.", "fallback", "simple"
    
    def generate_filename(self) -> str:
        """Generate a sequential filename"""
        return f"{random.randint(10000000000, 99999999999)}"
    
    def _generate_single_base_image(self, index: int) -> dict:
        try:
            # 1. Ä‘á»c toÃ n bá»™ Label.txt
            with open(self.label_file, "r", encoding="utf-8") as f:
                lines = [l for l in f.readlines() if l.strip()]

            if not lines:
                raise RuntimeError("Label.txt is empty")

            line = random.choice(lines)
            img_path, boxes = parse_label_line(line)

            # 3. random preset + style
            preset = random.randint(1, 8)

            base_image = generate_base_image(
                preset=preset,
                label_boxes=boxes
            )

            if base_image is None:
                raise RuntimeError("generate_base_image returned None")

            # 4. filename KHÃ”NG Ä‘á»¥ng nhau
            filename = f"{Path(img_path).stem}_{index}_{preset}"

            # 5. save image
            image_path = self.clean_dir / f"{filename}.jpg"
            base_image.save(image_path, format="JPEG", quality=95)

            # 6. save GT
            gt_path = self.gt_dir / f"{filename}.txt"
            with open(gt_path, "w", encoding="utf-8") as f:
                for box in boxes:
                    t = box["transcription"].strip()
                    if t:
                        f.write(t + "\n")

            return {
                "image": base_image,
                "text": "\n".join(b["transcription"] for b in boxes),
                "filename": filename,
                "source_file": img_path,
                "success": True,
                "index": index
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "index": index
            }


    
    def generate_base_images(self, num_images: int):
        """Generate base clean images in parallel and store them for noise processing"""
        print(f"Generating {num_images} base images using {self.max_workers} workers...")
        
        self.base_images = []  # Reset base images list
        success_count = 0
        
        # Use ThreadPoolExecutor for parallel generation
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks
            future_to_index = {
                executor.submit(self._generate_single_base_image, i): i 
                for i in range(num_images)
            }
            
            # Process results as they complete
            with tqdm(total=num_images, desc="Base images", unit="img") as pbar:
                for future in concurrent.futures.as_completed(future_to_index):
                    result = future.result()
                    
                    if result['success']:
                        # Thread-safe append to base_images list
                        with self.lock:
                            self.base_images.append({
                                'image': result['image'],
                                'text': result['text'],
                                'filename': result['filename'],
                                'source_file': result['source_file']
                            })
                        success_count += 1
                        pbar.set_postfix({'successful': success_count})
                    else:
                        pbar.write(f"âœ— Failed to generate base image {result['index'] + 1}: {result['error']}")
                    
                    pbar.update(1)
        
        print(f"Base image generation completed: {success_count}/{num_images} successful")
        return success_count
    
    def _generate_single_noisy_variant(self, base_data: dict, level: str) -> dict:
        """Generate a single noisy variant (for parallel execution)"""
        try:
            base_image = base_data['image']
            text_content = base_data['text']
            base_filename = base_data['filename']
            
            # Add noise to the base image
            noisy_image = add_noise_and_reduce_resolution(base_image, preset=level)
            
            # 10% chance of binarization
            if random.random() < 0.1:
                noisy_image = binarize_image(noisy_image)
            
            # Save noisy image
            # noisy_filename = f"{base_filename}_{level}"
            noisy_filename = f"{base_filename}"
            image_path = self.noisy_dir / f"{noisy_filename}.jpg"
            noisy_image.save(image_path, format="JPEG", quality=85)
            
            # Save ground truth (same as base image)
            gt_path = self.gt_dir / f"{noisy_filename}.txt"
            with open(gt_path, 'w', encoding='utf-8') as f:
                f.write(text_content)
            
            return {
                'success': True,
                'filename': noisy_filename,
                'level': level,
                'base_filename': base_filename
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'level': level,
                'base_filename': base_data.get('filename', 'unknown')
            }
    
    def generate_noisy_variants(self):
        """Generate noisy variants from the base images in parallel"""
        if not self.base_images:
            print("No base images available for noise processing")
            return 0
        
        # noise_levels = ["1_level", "2_level", "3_level", "4_level"]
        noise_levels = ["3_level"]
        total_success = 0
        
        # Create all tasks (base_image, level) combinations
        tasks = []
        for base_data in self.base_images:
            for level in noise_levels:
                tasks.append((base_data, level))
        
        total_tasks = len(tasks)
        print(f"\nGenerating {total_tasks} noisy variants from {len(self.base_images)} base images using {self.max_workers} workers...")
        
        # Use ThreadPoolExecutor for parallel generation
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks
            future_to_task = {
                executor.submit(self._generate_single_noisy_variant, base_data, level): (base_data, level)
                for base_data, level in tasks
            }
            
            # Process results as they complete
            with tqdm(total=total_tasks, desc="Noisy variants", unit="img") as pbar:
                for future in concurrent.futures.as_completed(future_to_task):
                    result = future.result()
                    
                    if result['success']:
                        total_success += 1
                        pbar.set_postfix({'successful': total_success})
                    else:
                        pbar.write(f"âœ— Failed to generate {result['level']} for {result['base_filename']}: {result['error']}")
                    
                    pbar.update(1)
        
        print(f"Noisy variant generation completed: {total_success}/{total_tasks} successful")
        return total_success

def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description="PreP-OCR Synthetic Data Generator")
    parser.add_argument("--base", type=int, default=5, 
                       help="Number of base images to generate (each produces 4 noisy variants)")
    parser.add_argument("--workers", type=int, default=None,
                       help="Number of parallel workers (default: auto-detect based on CPU cores)")
    
    args = parser.parse_args()
    
    if args.base <= 0:
        print("Please specify a positive number of base images to generate.")
        print("Example: python generate_ocr_data.py --base 5")
        return 1
    
    try:
        generator = OCRDataGenerator(max_workers=args.workers)
        
        # Step 1: Generate base images
        base_count = generator.generate_base_images(args.base)
        
        if base_count == 0:
            print("âŒ No base images were generated successfully.")
            return 1
        
        # Step 2: Generate noisy variants from base images
        noisy_count = generator.generate_noisy_variants()
        
        print(f"\nðŸŽ‰ Data generation completed!")
        print(f"ðŸ“Š Results:")
        print(f"  - Base images: {base_count}")
        print(f"  - Noisy variants: {noisy_count}")
        print(f"  - Total images: {base_count + noisy_count}")
        print(f"  - Ground truth files: {base_count + noisy_count}")
        print(f"\nðŸ“ Output directory: {generator.output_folder}")
        print(f"  - Clean images: {generator.clean_dir}")
        print(f"  - Noisy images: {generator.noisy_dir}")
        print(f"  - Ground truth: {generator.gt_dir}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ Generation failed: {e}")
        return 1

if __name__ == "__main__":
    exit(main())